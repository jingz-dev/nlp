{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep (Fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_fib_seq(n, seq_len=4):\n",
    "    fib_seq = [1, 1]\n",
    "    for _ in range(100):\n",
    "        fib_seq.append(fib_seq[-1] + fib_seq[-2])\n",
    "    dataset = []\n",
    "    for _ in range(n):\n",
    "        i = random.randint(2, 99)\n",
    "        seq = [fib_seq[i-1], fib_seq[i]]\n",
    "        for _ in range(seq_len - 2):\n",
    "            seq.append(seq[-1] + seq[-2])\n",
    "        dataset.append(seq)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(generate_fib_seq(30)).astype(float)\n",
    "training_set = dataset[:20]\n",
    "val_set = dataset[20:25]\n",
    "test_set = dataset[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "training_set_log_scaled = np.log(training_set)\n",
    "scaler = MinMaxScaler()\n",
    "training_set_processed = scaler.fit_transform(training_set_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "training_tensor = torch.tensor(training_set_log_scaled).float()\n",
    "x_train, y_train = training_tensor.split(3, dim=1)\n",
    "\n",
    "dataset = TensorDataset(x_train.unsqueeze(2), y_train) # train: N, 3, 1, test: N, 1\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f'input shape {x.shape}')\n",
    "        _, x = self.rnn.forward(x)\n",
    "        # output, h_t = _, x\n",
    "        # print(f'output shape {output.shape}, h_t shape {h_t.shape}')\n",
    "        x = self.linear.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_2306/2417774145.py:10: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([1, 4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_pred, y_batch)\n",
      "100%|██████████| 50/50 [00:00<00:00, 211.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = BasicRNN(1, 30)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epoch = 50\n",
    "for _ in tqdm(range(epoch)):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = F.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[360.75494]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.forward(torch.log(torch.tensor([89, 144, 233]).unsqueeze(1).float()))\n",
    "    print(np.exp(y_pred.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_log_scaled = np.log(test_set)\n",
    "test_tensor = torch.tensor(test_set_log_scaled).float()\n",
    "x_test, y_test = test_tensor.split(3, dim=1)\n",
    "x_test = x_test.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 1])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.823124e+33\n"
     ]
    }
   ],
   "source": [
    "model.eval()    \n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "\n",
    "y_pred_np = y_pred.squeeze(0).numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(np.exp(y_test_np), np.exp(y_pred_np))\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attn_score = a(s_t-1, h_i) where a is some function\n",
    "\n",
    "weights_t,i = softmax(e_t,i)\n",
    "\n",
    "context_vector_t =  sum(weights*h_i)\n",
    "'''\n",
    "debug_attention = None\n",
    "debug_encoder_outputs = None\n",
    "debug_h_t = None\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, h_t):\n",
    "        global debug_attention\n",
    "        # encoder_output: (N, L, H), h_t: (N, 1, H)\n",
    "        e = F.relu(self.W1(encoder_outputs) + self.W2(h_t)) # (N, L, H) + (N, 1, H)|broadcast\n",
    "        score = self.V(e) # V.shape (N, L, 1)\n",
    "        debug_attention = F.softmax(score, dim=1)\n",
    "        return debug_attention\n",
    "        # return F.softmax(attention, dim=1)\n",
    "\n",
    "class DotProductAttentionLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, encoder_outputs, h_t):\n",
    "        # encoder_outputs (N, L, H)\n",
    "        # h_t (N, 1, H)\n",
    "        score = torch.bmm(encoder_outputs, h_t.transpose(1, 2)) # (N, L, 1)\n",
    "        a = F.softmax(score, dim=1)\n",
    "        global debug_attention, debug_encoder_outputs, debug_h_t\n",
    "        debug_attention = a\n",
    "        debug_encoder_outputs = encoder_outputs\n",
    "        debug_h_t = h_t\n",
    "        return a\n",
    "\n",
    "class BasicRNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.attention = DotProductAttentionLayer()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, h_t = self.rnn.forward(x) \n",
    "        # attention is applied post RNN. it does not fully leverage the potential of integrating attention directly into the RNN computation\n",
    "        # calculating attention at each timestep requires custom handling of timesteps in x_batch[i]\n",
    "        h_t = h_t.transpose(0, 1)\n",
    "        attention_weights = self.attention.forward(outputs, h_t) # (N, L, 1)\n",
    "        # attn_w (N, L, 1)   x (N, L, H)\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), outputs) # N, 1, H\n",
    "        context_vector = context_vector.squeeze(1)\n",
    "        x = self.linear.forward(context_vector)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensor = torch.tensor(np.log(training_set)).float()\n",
    "x_train, y_train = training_tensor.split(3, dim=1)\n",
    "\n",
    "dataset = TensorDataset(x_train.unsqueeze(2), y_train)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x_val, y_val = torch.tensor(np.log(val_set)).float().split(3, dim=1)\n",
    "x_val = x_val.unsqueeze(2)\n",
    "\n",
    "x_test, y_test = torch.tensor(np.log(test_set)).float().split(3, dim=1)\n",
    "x_test = x_test.unsqueeze(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensor = torch.tensor(training_set).float()\n",
    "x_train, y_train = training_tensor.split(3, dim=1)\n",
    "\n",
    "dataset = TensorDataset(x_train.unsqueeze(2), y_train)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x_val, y_val = torch.tensor(val_set).float().split(3, dim=1)\n",
    "x_val = x_val.unsqueeze(2)\n",
    "\n",
    "x_test, y_test = torch.tensor(test_set).float().split(3, dim=1)\n",
    "x_test = x_test.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 163.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BasicRNNWithAttention(1, 7)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epoch = 1000\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def compute_log_val_loss(model, x_val, y_val):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.forward(x_val)\n",
    "        loss = F.mse_loss(torch.log(y_pred), torch.log(y_val))\n",
    "        return loss.item()\n",
    "\n",
    "for e in tqdm(range(epoch)):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = F.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_loss = compute_log_val_loss(model, x_val, y_val)\n",
    "    # print(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        print(f'early stopping at epoch {e}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[468.68658]]\n"
     ]
    }
   ],
   "source": [
    "seq = [89, 144, 233]\n",
    "x_input = torch.tensor(seq).reshape(1, 3, 1).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model.forward(torch.log(x_input))\n",
    "    print(np.exp(y_pred.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.10379\n"
     ]
    }
   ],
   "source": [
    "model.eval()    \n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "\n",
    "y_pred_np = y_pred.squeeze(0).numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test_np, y_pred_np)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN with attention for predicting fibonacci numbers notes\n",
    "  * Given the large variance in scale, from 1 to billions, numerical stability becomes a problem\n",
    "    * log scaling the input\n",
    "    * gradient clipping\n",
    "  * recurring activations push hidden layers towards 1 or -1, even after log scaled. Not much is learned.\n",
    "  * low numbers of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('rnn.weight_ih_l0',\n",
       "              tensor([[ 0.0253],\n",
       "                      [ 0.0096],\n",
       "                      [-0.3131],\n",
       "                      [-0.0291],\n",
       "                      [-0.0164],\n",
       "                      [-0.0147],\n",
       "                      [ 0.2367]])),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[ 0.2606,  0.3990, -0.0763, -0.2116, -0.0966,  0.4863,  0.5038],\n",
       "                      [ 0.1999,  0.1977, -0.5518, -0.3686,  0.1758, -0.1812,  0.0813],\n",
       "                      [ 0.3376, -0.3192, -0.1182,  0.1830,  0.0291,  0.2075, -0.0565],\n",
       "                      [ 0.3539, -0.3627,  0.0881,  0.0058, -0.3877, -0.0663, -0.5361],\n",
       "                      [-0.4692, -0.1756,  0.5350,  0.0281, -0.3063, -0.0777, -0.3484],\n",
       "                      [-0.1327, -0.3040,  0.2115,  0.4709, -0.1651, -0.3135, -0.0269],\n",
       "                      [ 0.1180,  0.3517, -0.1059,  0.0586,  0.1548,  0.1565,  0.1477]])),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([ 0.3482,  0.5256, -0.0538, -0.5636, -0.4936,  0.1212, -0.1911])),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([ 0.4040, -0.0715, -0.1207, -0.3666, -0.5724, -0.5691,  0.3789])),\n",
       "             ('linear.weight',\n",
       "              tensor([[ 0.1940,  0.2031, -0.1922, -0.3124, -0.2046, -0.2973,  0.1128]])),\n",
       "             ('linear.bias', tensor([-0.0306]))])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4135],\n",
       "         [ 6.8947],\n",
       "         [ 7.3759]],\n",
       "\n",
       "        [[29.0304],\n",
       "         [29.5116],\n",
       "         [29.9928]],\n",
       "\n",
       "        [[ 1.0986],\n",
       "         [ 1.6094],\n",
       "         [ 2.0794]],\n",
       "\n",
       "        [[45.3916],\n",
       "         [45.8728],\n",
       "         [46.3540]],\n",
       "\n",
       "        [[25.1807],\n",
       "         [25.6619],\n",
       "         [26.1431]]])"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9385,  0.7197,  0.9772,  0.9476, -0.9605,  0.9699, -0.5150],\n",
       "         [ 0.9717, -0.9840,  0.7426,  0.0812, -0.6823,  0.9971,  0.9998],\n",
       "         [ 0.7367, -0.9138,  0.6385, -0.2419, -0.9479,  0.9969,  0.9990]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000],\n",
       "         [ 1.0000,  0.9992,  1.0000,  1.0000, -1.0000,  1.0000, -0.9839],\n",
       "         [ 1.0000,  0.9994,  1.0000,  1.0000, -1.0000,  1.0000, -0.9876]],\n",
       "\n",
       "        [[ 0.4824, -0.5419,  0.6573,  0.4467, -0.2499,  0.8659,  0.7758],\n",
       "         [ 0.1869, -0.9881, -0.4207, -0.8575, -0.0675,  0.9683,  0.9998],\n",
       "         [-0.3154, -0.2252,  0.3518, -0.0150, -0.6101,  0.9236,  0.8786]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  0.9999, -1.0000],\n",
       "         [ 1.0000,  0.9932,  1.0000,  0.9999, -1.0000,  1.0000, -0.8466],\n",
       "         [ 1.0000,  0.9945,  1.0000,  0.9999, -1.0000,  1.0000, -0.8568]]])"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7367, -0.9138,  0.6385, -0.2419, -0.9479,  0.9969,  0.9990]],\n",
       "\n",
       "        [[ 1.0000,  0.9994,  1.0000,  1.0000, -1.0000,  1.0000, -0.9876]],\n",
       "\n",
       "        [[-0.3154, -0.2252,  0.3518, -0.0150, -0.6101,  0.9236,  0.8786]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.9945,  1.0000,  0.9999, -1.0000,  1.0000, -0.8568]]])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0260],\n",
       "         [0.4809],\n",
       "         [0.4932]],\n",
       "\n",
       "        [[0.3366],\n",
       "         [0.3310],\n",
       "         [0.3323]],\n",
       "\n",
       "        [[0.2800],\n",
       "         [0.2839],\n",
       "         [0.4361]],\n",
       "\n",
       "        [[0.3333],\n",
       "         [0.3333],\n",
       "         [0.3333]],\n",
       "\n",
       "        [[0.3635],\n",
       "         [0.3166],\n",
       "         [0.3198]]])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 7\n",
    "W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "V = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "rnn = nn.RNN(1, hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h_t = rnn.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(output).shape=torch.Size([10, 3, 7])\n",
      "W2(h_t.transpose(0, 1)).shape=torch.Size([10, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f'{W1(output).shape=}')\n",
    "print(f'{W2(h_t.transpose(0, 1)).shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (W1(output) + W2(h_t.transpose(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_w = F.softmax(V(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_w.shape=torch.Size([10, 3, 1])\n",
      "h_t.transpose(0, 1).shape=torch.Size([10, 1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 7])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'{attn_w.shape=}') # N, L, 1\n",
    "print(f'{h_t.transpose(0, 1).shape=}') # N, 1, H\n",
    "torch.bmm(attn_w.transpose(1, 2), output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
